[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning for Bioinformatics",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "01_intro.html",
    "href": "01_intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What is Machine Learning?\nMachine Learning is a field of study within computer science that gives computers the ability to learn without being explicitly programmed to perform specific tasks. Instead of following a set of predefined rules, ML algorithms identify patterns in data and use these patterns to make predictions or decisions. ML was first coined by Arthur Samuel in 1959, who defined it as “the field of study that gives computers the ability to learn without being explicitly programmed.” Even though ML has been around for decades, it has gained significant traction in recent years due to advancements in computational power, the availability of large datasets, and improvements in algorithms. ML uses statistical techniques to enable computers to improve their performance on a specific task over time as they are exposed to more data (Figure 1.1). This is particularly useful in scenarios where traditional programming approaches are impractical or impossible due to the complexity of the data or the task at hand.\nFigure 1.1: The Landscape of Machine Learning in Biology",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#why-machine-learning-in-biology",
    "href": "01_intro.html#why-machine-learning-in-biology",
    "title": "1  Introduction",
    "section": "1.2 Why Machine Learning in Biology?",
    "text": "1.2 Why Machine Learning in Biology?\nBiological data is inherently complex and high-dimensional. Traditional univariate statistical methods often fall short in capturing the intricate relationships within such data. Machine Learning offers a suite of tools that can handle this complexity, allowing us to:\n\nIdentify Patterns: ML algorithms can uncover hidden patterns in large datasets that may not be apparent through traditional analysis.\nMake Predictions: ML models can predict outcomes based on input data, which is invaluable for tasks such as disease diagnosis, treatment response prediction, and more.\nAutomate Analysis: ML can automate the analysis of large datasets, saving time and resources.\nIntegrate Diverse Data Types: ML techniques can integrate various data types (e.g., genomic, proteomic, clinical) to provide a holistic view of biological systems.\n\nBy leveraging Machine Learning, we can enhance our ability to make sense of complex biological data and drive novel discoveries in the life sciences.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02_ml_basics.html",
    "href": "02_ml_basics.html",
    "title": "2  The Core Workflow of ML & Biological Pre-processing",
    "section": "",
    "text": "2.1 The ML Workflow Overview\nA robust ML pipeline for biological data follows these non-negotiable steps:\nFor our course, we will use the TCGA Cholangiocarcinoma (CHOL) cohort. We will use the {TCGAbiolinks} package to download and prepare the data.\n# Prepare our Data for Machine Learning with \n# TCGA Cholangiocarcinoma (CHOL) Cohort\nlibrary(TCGAbiolinks)\nlibrary(SummarizedExperiment)\nlibrary(tidyverse)\n\n# 1. Query only the Cholangiocarcinoma cohort\nquery_chol &lt;- GDCquery(\n  project = \"TCGA-CHOL\",\n  data.category = \"Transcriptome Profiling\",\n  data.type = \"Gene Expression Quantification\",\n  workflow.type = \"STAR - Counts\"\n)\n#  Load the data into a SummarizedExperiment object\nchol_se &lt;- GDCprepare(query_chol) \n\n\n|                                                    |  0%                      \n|=                                                   |2.272727% ~2 s remaining  \n|==                                                  |4.545455% ~2 s remaining  \n|===                                                 |6.818182% ~2 s remaining  \n|====                                                |9.090909% ~2 s remaining  \n|=====                                               |11.36364% ~2 s remaining  \n|=======                                             |13.63636% ~1 s remaining  \n|========                                            |15.90909% ~1 s remaining  \n|=========                                           |18.18182% ~3 s remaining  \n|==========                                          |20.45455% ~2 s remaining  \n|===========                                         |22.72727% ~2 s remaining  \n|=============                                       | 25% ~2 s remaining       \n|==============                                      |27.27273% ~2 s remaining  \n|===============                                     |29.54545% ~2 s remaining  \n|================                                    |31.81818% ~2 s remaining  \n|=================                                   |34.09091% ~2 s remaining  \n|==================                                  |36.36364% ~2 s remaining  \n|====================                                |38.63636% ~1 s remaining  \n|=====================                               |40.90909% ~2 s remaining  \n|======================                              |43.18182% ~2 s remaining  \n|=======================                             |45.45455% ~2 s remaining  \n|========================                            |47.72727% ~1 s remaining  \n|==========================                          | 50% ~1 s remaining       \n|===========================                         |52.27273% ~1 s remaining  \n|============================                        |54.54545% ~1 s remaining  \n|=============================                       |56.81818% ~1 s remaining  \n|==============================                      |59.09091% ~1 s remaining  \n|===============================                     |61.36364% ~1 s remaining  \n|=================================                   |63.63636% ~1 s remaining  \n|==================================                  |65.90909% ~1 s remaining  \n|===================================                 |68.18182% ~1 s remaining  \n|====================================                |70.45455% ~1 s remaining  \n|=====================================               |72.72727% ~1 s remaining  \n|=======================================             | 75% ~1 s remaining       \n|========================================            |77.27273% ~1 s remaining  \n|=========================================           |79.54545% ~1 s remaining  \n|==========================================          |81.81818% ~0 s remaining  \n|===========================================         |84.09091% ~0 s remaining  \n|============================================        |86.36364% ~0 s remaining  \n|==============================================      |88.63636% ~0 s remaining  \n|===============================================     |90.90909% ~0 s remaining  \n|================================================    |93.18182% ~0 s remaining  \n|=================================================   |95.45455% ~0 s remaining  \n|==================================================  |97.72727% ~0 s remaining  \n|====================================================|100% ~0 s remaining       \n|====================================================|100%                      Completed after 2 s \n\n# 2. Download and prepare the data\n# Extract TPM using \ntpm_data &lt;- assay(chol_se, \"tpm_unstrand\")\n\n# 3. Quick Metadata Link\nmetadata &lt;- as.data.frame(colData(chol_se)) %&gt;%\n  select(barcode, sample_type)\nhead(metadata)\n\n                                                  barcode         sample_type\nTCGA-W5-AA39-01A-11R-A41I-07 TCGA-W5-AA39-01A-11R-A41I-07       Primary Tumor\nTCGA-3X-AAVB-01A-31R-A41I-07 TCGA-3X-AAVB-01A-31R-A41I-07       Primary Tumor\nTCGA-W5-AA2R-11A-11R-A41I-07 TCGA-W5-AA2R-11A-11R-A41I-07 Solid Tissue Normal\nTCGA-W5-AA38-01A-11R-A41I-07 TCGA-W5-AA38-01A-11R-A41I-07       Primary Tumor\nTCGA-W5-AA2G-01A-11R-A41I-07 TCGA-W5-AA2G-01A-11R-A41I-07       Primary Tumor\nTCGA-W5-AA2Q-11A-11R-A41I-07 TCGA-W5-AA2Q-11A-11R-A41I-07 Solid Tissue Normal\n\n# 4. ML Preparation (Transpose)\n# Often in ML, TPM or FPKM values are used directly\n# We need the samples as rows and genes as columns\n# We filter for low-expression genes to remove noise\n# For TPM, a common threshold is &gt; 0.1 or &gt; 1 in a certain % of samples\nkeep &lt;- rowSums(tpm_data &gt; 1) &gt;= 5\ntpm_filtered &lt;- tpm_data[keep, ]\ndim(tpm_filtered) # Check dimensions after filtering\n\n[1] 22215    44\n\nhead(tpm_filtered[, 1:5]) # View first 5 samples for first few genes\n\n                   TCGA-W5-AA39-01A-11R-A41I-07 TCGA-3X-AAVB-01A-31R-A41I-07\nENSG00000000003.15                      31.4284                      33.0598\nENSG00000000419.13                      53.0852                      84.5487\nENSG00000000457.14                       7.8177                       5.1603\nENSG00000000460.17                       2.9002                       2.4024\nENSG00000000938.13                       1.6248                      10.6799\nENSG00000000971.16                     543.8357                      33.0199\n                   TCGA-W5-AA2R-11A-11R-A41I-07 TCGA-W5-AA38-01A-11R-A41I-07\nENSG00000000003.15                      46.9606                     133.6240\nENSG00000000419.13                      35.9064                     103.3878\nENSG00000000457.14                       2.4906                       4.1205\nENSG00000000460.17                       0.5763                       2.6406\nENSG00000000938.13                       2.9092                       3.9425\nENSG00000000971.16                     465.5344                       4.0710\n                   TCGA-W5-AA2G-01A-11R-A41I-07\nENSG00000000003.15                      29.4144\nENSG00000000419.13                      80.7108\nENSG00000000457.14                       6.0720\nENSG00000000460.17                       2.2160\nENSG00000000938.13                       8.4903\nENSG00000000971.16                     143.9028\n\n## Combine with metadata\ndf_ml_tpm &lt;- as.data.frame(t(tpm_filtered)) %&gt;%\n  rownames_to_column(\"barcode\") %&gt;%\n  inner_join(metadata, by = \"barcode\") %&gt;%\n  select(-barcode) %&gt;%\n  mutate(sample_type = factor(sample_type))\ndim(df_ml_tpm) # Check final dimensions\n\n[1]    44 22216",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Core Workflow of ML & Biological Pre-processing</span>"
    ]
  },
  {
    "objectID": "03_ml_supervised.html",
    "href": "03_ml_supervised.html",
    "title": "3  Supervised Machine Learning",
    "section": "",
    "text": "3.1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supervised Machine Learning</span>"
    ]
  },
  {
    "objectID": "04_ml_unsupervised.html",
    "href": "04_ml_unsupervised.html",
    "title": "4  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "05_ml_deep_learning.html",
    "href": "05_ml_deep_learning.html",
    "title": "5  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "06_ml_evaluation.html",
    "href": "06_ml_evaluation.html",
    "title": "6  Evaluating Machine Learning Models",
    "section": "",
    "text": "6.1 Cross Validation",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evaluating Machine Learning Models</span>"
    ]
  },
  {
    "objectID": "01_intro.html#course-structure",
    "href": "01_intro.html#course-structure",
    "title": "1  Introduction",
    "section": "1.3 Course Structure",
    "text": "1.3 Course Structure\nThis course is structured into several modules, each focusing on different aspects of Machine Learning:\n\nSupervised Learning: Techniques where the model is trained on labeled data to make predictions.\nUnsupervised Learning: Methods for discovering patterns in unlabeled data.\nDeep Learning: An introduction to neural networks and their applications in biology.\nModel Evaluation: Strategies for assessing the performance of ML models.\n\nEach module will include theoretical concepts, practical examples, and hands-on exercises using R. We will also explore real-world biological datasets to illustrate the application of ML techniques in research.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#types-of-machine-learning",
    "href": "01_intro.html#types-of-machine-learning",
    "title": "1  Introduction",
    "section": "1.4 Types of Machine Learning",
    "text": "1.4 Types of Machine Learning\nMachine Learning can be broadly categorized into three main types: Supervised Learning, Unsupervised Learning, and Reinforcement Learning, as depicted in Figure 1.2. In this course, we will focus primarily on the first two types, as they are most relevant to biological data analysis.\n\nSupervised Learning: In supervised learning, the model is trained on a labeled dataset, meaning that each input data point is paired with a corresponding output label. The goal is for the model to learn the mapping from inputs to outputs so that it can make accurate predictions on new, unseen data. Common algorithms include decision trees, support vector machines, and neural networks. Applications in biology include disease classification based on gene expression profiles and predicting protein functions. We will explore various supervised learning techniques in detail in Chapter 4.\nUnsupervised Learning: Unsupervised learning involves training models on unlabeled data to identify inherent patterns or structures within the data. The model learns to group similar data points together or to reduce the dimensionality of the data for easier visualization and analysis. Common techniques include clustering algorithms (like k-means and hierarchical clustering) and dimensionality reduction methods (like PCA and t-SNE). In biology, unsupervised learning can be used for tasks such as identifying cell types in single-cell RNA sequencing data or discovering subtypes of diseases based on molecular profiles. We will cover unsupervised learning methods in detail in Chapter 3.\nReinforcement Learning: Although not the focus of this course, reinforcement learning is another type of ML where an agent learns to make decisions by taking actions in an environment to maximize cumulative rewards. This approach is less commonly used in biological data analysis but has potential applications in areas like drug discovery and personalized medicine.\n\n\n\n\n\n\n\n\n\nFigure 1.2: The Three Main Types of Machine Learning: Supervised, Unsupervised, and Reinforcement Learning.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#methodological-challenges-in-biological-machine-learning",
    "href": "01_intro.html#methodological-challenges-in-biological-machine-learning",
    "title": "1  Introduction",
    "section": "1.5 Methodological Challenges in Biological Machine Learning",
    "text": "1.5 Methodological Challenges in Biological Machine Learning\nWhile Machine Learning offers powerful tools for biological data analysis, several methodological challenges must be addressed to ensure robust and meaningful results. Below, we outline some of the key challenges specific to the application of ML in biology.\n\n1.5.1 Data Quality and Preprocessing\nBiological datasets often suffer from issues such as missing values, batch effects, and technical variability. Proper data preprocessing is crucial to mitigate these issues.\n\nNormalization and Scaling: Techniques such as log transformation, z-score normalization, and batch effect correction (e.g., ComBat) are essential to ensure that the data is comparable across samples and conditions.\nFeature Selection: Given the high dimensionality of biological data, selecting relevant features (e.g., genes, proteins) is critical to reduce noise and improve model performance. Methods such as variance thresholding, recursive feature elimination, and domain knowledge-based selection can be employed.\n\n\n\n1.5.2 The “Curse of Dimensionality” and Data Scarcity\nIn computational biology, we frequently encounter the \\(p \\gg n\\) problem, where the number of features (genes/ transcripts/ proteins/ metabolites) vastly exceeds the number of observations (biological replicates/ individuals).\n\nDimensionality Concerns: As the feature space expands, the volume of the space increases so rapidly that the available data becomes sparse. This sparsity makes it mathematically trivial to find a separating hyperplane that appears significant but lacks biological reality. Often, models trained in such high-dimensional spaces will perform well on training data but fail to generalize to new samples.\nQuality Constraints: Biological data is inherently noisy, susceptible to batch effects, and expensive to generate. Models trained on insufficient or poorly normalized data will inevitably reflect technical artifacts rather than true physiological signals. Moreover, the limited sample sizes typical in biological studies exacerbate the risk of spurious correlations.\n\n\n\n1.5.3 Overfitting and Generalization Error\nOverfitting represents the most significant threat to the transition from computational predictions to biological insights. Overfitting occurs when a model possesses excessive degrees of freedom, allowing it to interpolate the stochastic noise within the training set rather than capturing the underlying biological manifold. In high-dimensional transcriptomics, where \\(p \\gg n\\), the risk of “memorizing” the training data is exceptionally high. Obviously, this is a critical issue in most of the biological discoveries, where the goal is to identify features that are predictive across diverse populations.\n\nThe Statistical Mechanism: In high-dimensional spaces, points are naturally isolated, we say that the data is sparse. A complex algorithm, such as an unpruned Decision Tree or a high-capacity Support Vector Machine, can easily construct a decision boundary that achieves zero training error by isolating these specific coordinates. However, such a boundary is unlikely to represent the true population distribution, leading to a precipitous drop in performance when applied to independent validation cohorts.\nRegularization and Constraint: To mitigate this, we must introduce penalization terms—such as the \\(L_1\\) (Lasso) or \\(L_2\\) (Ridge) norms—which mathematically constrain the coefficient weights, effectively “shrinking” less informative features to zero. This enforces a simpler model architecture that prioritizes broad biological trends over sample-specific idiosyncrasies. We will explore these techniques in detail in Section 4.2.\nRigorous Validation Frameworks: The antidote to overfitting is a robust validation architecture. This involves the strict separation of data into training, testing, and ideally, completely independent external validation sets. Different cross-validation strategies (e.g., k-fold, stratified) should be employed to ensure that the model’s performance metrics (e.g., accuracy, AUC-ROC) reflect its ability to generalize beyond the training data. We will discuss these strategies in detail in Chapter 6.\nThe Biological Cost of High Variance: From a translational perspective, an overfitted model produces “phantom biomarkers.” These are genes that appear statistically significant due to technical artifacts (e.g., batch effects or library prep variance) but fail to replicate in follow-up experiments. Identifying these errors early via ML validation saves significant time and capital that would otherwise be spent on futile in vitro or in vivo validation.\n\n\n\n1.5.4 The Interpretability-Complexity Paradigm\nThe application of Machine Learning in omics often forces a trade-off between a model’s predictive capacity and its interpretability. While high-capacity models—such as Gradient Boosted Trees, Random Forests, or Deep Neural Networks—can capture complex, non-linear gene-gene interactions, they often function as “black boxes,” providing little intuition regarding the biological phenomena driving the classification.\n\nMechanistic Transparency vs. Predictive Accuracy: In a clinical research context, the rationale behind a prediction is often as critical as the prediction itself. A model that achieves high AUC (Area Under the Curve) but lacks transparency cannot be used to generate new biological hypotheses or inform therapeutic interventions. To bridge this gap, we employ explainable AI (XAI) techniques, such as SHAP (SHapley Additive exPlanations) or LIME, which mathematically decompose the contribution of each gene to a specific sample’s prediction.\nStrategic Model Selection: The choice of algorithm must be dictated by the research objective. If the goal is pure diagnostic accuracy, ensemble methods are appropriate. However, if the objective is biomarker identification, we must prioritize models with intrinsic feature selection properties (e.g., Sparse Partial Least Squares or Elastic Net). This allows us to map mathematical importance back to biological relevance—transforming “important features” into actionable “biomarkers” that can be validated via pathway enrichment or Gene Ontology (GO) analysis.\nThe Burden of Validation: A complex, uninterpretable model is difficult to audit for “leakage” or technical bias. By maintaining interpretability, we ensure that the model is making decisions based on genuine biological signal (e.g., the upregulation of an oncogenic pathway) rather than a technical artifact (e.g., different RNA extraction kits), thereby increasing the likelihood of successful translation to the wet lab.\n\n\n\n1.5.5 Computational Logistics and Scalability\nThe processing of large-scale omics datasets requires significant memory allocation and optimized algorithmic implementations.\n\nInfrastructural Demands: Complex architectures, particularly in deep learning or iterative manifold learning (UMAP/t-SNE), necessitate high-performance computing (HPC) environments. Researchers must evaluate the trade-off between algorithmic complexity and the marginal gain in predictive power.\n\n\n\n1.5.6 Algorithmic Bias and Ethical Validity\nThe translation of Machine Learning models into clinical diagnostics needs a profound commitment to data diversity and ethical transparency.\n\nRepresentation Bias: If the training data lacks ancestral or demographic diversity, the resulting selected genes may exhibit reduced efficacy across broader populations, inadvertently perpetuating healthcare disparities.\nAccountability: The use of automated decision-making in healthcare requires rigorous auditing of the training pipeline to ensure that confounding variables (e.g., age, sex, or technical batch) are not driving the model’s classification.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "03_ml_supervised.html#sec-regularization",
    "href": "03_ml_supervised.html#sec-regularization",
    "title": "3  Supervised Machine Learning",
    "section": "3.2 Regularization Techniques",
    "text": "3.2 Regularization Techniques",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supervised Machine Learning</span>"
    ]
  },
  {
    "objectID": "01_intro.html#sec-why-ml-bio",
    "href": "01_intro.html#sec-why-ml-bio",
    "title": "1  Introduction",
    "section": "1.2 Why Machine Learning in Biology?",
    "text": "1.2 Why Machine Learning in Biology?\nBiological data is inherently complex and high-dimensional. Traditional univariate statistical methods often fall short in capturing the intricate relationships within such data. Machine Learning offers a suite of tools that can handle this complexity, allowing us to:\n\nIdentify Patterns: ML algorithms can uncover hidden patterns in large datasets that may not be apparent through traditional analysis.\nMake Predictions: ML models can predict outcomes based on input data, which is invaluable for tasks such as disease diagnosis, treatment response prediction, and more.\nAutomate Analysis: ML can automate the analysis of large datasets, saving time and resources.\nIntegrate Diverse Data Types: ML techniques can integrate various data types (e.g., genomic, proteomic, clinical) to provide a holistic view of biological systems.\n\nBy leveraging Machine Learning, we can enhance our ability to make sense of complex biological data and drive novel discoveries in the life sciences.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02_ml_basics.html#sec-data-spending",
    "href": "02_ml_basics.html#sec-data-spending",
    "title": "2  The Core Workflow of ML & Biological Pre-processing",
    "section": "2.2 The Spending Plan: Data Splitting Strategies",
    "text": "2.2 The Spending Plan: Data Splitting Strategies\nIn biology, samples are precious and often limited (\\(n &lt; 100\\)). This scarcity makes the “Data Spending” phase the most high-stakes part of your project. If you use your data too aggressively during training, you will overfit; if you save too much for testing, your model will be too weak to learn the underlying biology. Finally, if you “peek” at the test set during pre-processing, you will introduce Data Leakage1, leading to overly optimistic performance estimates.\n\n2.2.1 The Initial Split: Training vs. Testing\nTo ensure that our model’s performance is a true reflection of its ability to generalize, we must carefully partition our dataset at the very beginning of our analysis. For that, we divide our dataset into two primary components (Figure 2.1):\n\nThe Training Set: This subset is used to train the model. All pre-processing steps (normalization, feature selection, imputation) must be derived solely from this set. Often, we use 70% to 80% of the data for training.\nThe Test Set: This subset is held out and only used once at the very end to evaluate the model’s performance. It must remain completely unseen during training and pre-processing.\n\n\nNote: Never perform normalization, feature selection, or imputation on the entire dataset before splitting. If you calculate the mean expression of a gene using all samples, the training set now “knows” something about the distribution of the test set. This is Data Leakage, and it leads to artificially inflated (and ultimately false) performance metrics.\n\n\n\n\n\n\n\n\n\nFigure 2.1: Data Spending: Training vs. Testing Sets\n\n\n\n\n\nThe easiest way to implement this split in R is using the initial_split() function from the {rsample} package, which is part of the tidymodels ecosystem.\n\n# Split the data into training and testing sets\nlibrary(tidymodels)\nset.seed(42) # For reproducibility\ndata_split &lt;- initial_split(df_ml_tpm, prop = 0.75)\ntrain_data_chol &lt;- training(data_split)\ntest_data_chol  &lt;- testing(data_split)\n\ncat(\"Training samples:\", nrow(train_data_chol), \"\\nTesting samples:\", nrow(test_data_chol))\n\nTraining samples: 33 \nTesting samples: 11\n\ncat(\"Proportion of sample types in training set:\\n\")\n\nProportion of sample types in training set:\n\nprint(prop.table(table(train_data_chol$sample_type)))\n\n\n      Primary Tumor Solid Tissue Normal \n          0.7878788           0.2121212 \n\ncat(\"Proportion of sample types in testing set:\\n\")\n\nProportion of sample types in testing set:\n\nprint(prop.table(table(test_data_chol$sample_type)))\n\n\n      Primary Tumor Solid Tissue Normal \n          0.8181818           0.1818182 \n\n\n\n\n2.2.2 Stratified Sampling\nIn several biological contexts, the classes we are trying to predict are imbalanced, which can lead to misleading performance metrics if not handled properly. Menaing that one class (e.g., healthy controls) may vastly outnumber another (e.g., patients with a rare disease). If we randomly split the data, we risk creating training and testing sets that do not accurately represent the overall class distribution. This can lead to models that perform well on the majority class but poorly on the minority class, which is often of greater interest in biological studies.\nTo address this, we use Stratified Sampling to ensure that the proportion of the outcome (e.g., disease status) is preserved in both the training and testing sets.\nIn R, we will use the same function as before, but with a new argument.\n\n# Split the data into training and testing sets\nlibrary(tidymodels)\nset.seed(42) # For reproducibility\ndata_split &lt;- initial_split(df_ml_tpm, prop = 0.75, strata = sample_type)\ntrain_data_chol &lt;- training(data_split)\ntest_data_chol  &lt;- testing(data_split)\n\ncat(\"Training samples:\", nrow(train_data_chol), \"\\nTesting samples:\", nrow(test_data_chol))\n\nTraining samples: 32 \nTesting samples: 12\n\ncat(\"Proportion of sample types in training set:\\n\")\n\nProportion of sample types in training set:\n\nprint(prop.table(table(train_data_chol$sample_type)))\n\n\n      Primary Tumor Solid Tissue Normal \n             0.8125              0.1875 \n\ncat(\"Proportion of sample types in testing set:\\n\")\n\nProportion of sample types in testing set:\n\nprint(prop.table(table(test_data_chol$sample_type)))\n\n\n      Primary Tumor Solid Tissue Normal \n               0.75                0.25",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Core Workflow of ML & Biological Pre-processing</span>"
    ]
  },
  {
    "objectID": "02_ml_basics.html#sec-recipes",
    "href": "02_ml_basics.html#sec-recipes",
    "title": "2  The Core Workflow of ML & Biological Pre-processing",
    "section": "2.3 2.2 The Recipe: Pre-processing Biological High-Dimensionality",
    "text": "2.3 2.2 The Recipe: Pre-processing Biological High-Dimensionality\nBiological data, particularly transcriptomics, is notoriously “messy.” It is characterized by:\n\nExtreme Scale Differences: Housekeeping genes may have counts in the 100,000s, while transcription factors reside in the 10s.\nHigh Sparsity: Many genes are not expressed in certain tissues (Zero-inflation).\nMulticollinearity: Genes act in pathways; if Gene A and Gene B are in the same complex, they are highly correlated, which can confuse many ML models.\n\n\n2.3.1 2.2.1 Feature Engineering with tidymodels\nWe use the recipes package to define a sequence of “steps.” This allows us to apply the exact same mathematical transformations to the test set that we derived from the training set.\nR\nbio_recipe &lt;- recipe(outcome ~ ., data = train_data) %&gt;%\n  # 1. Handle missingness (Impute using K-Nearest Neighbors)\n  step_impute_knn(all_predictors(), neighbors = 5) %&gt;%\n  \n  # 2. Remove Near-Zero Variance (NZV)\n  # Genes that don't change across samples are noise\n  step_nzv(all_predictors()) %&gt;%\n  \n  # 3. Handle Skewness\n  # Biological counts are often log-normally distributed\n  step_log(all_outcomes(), offset = 1) %&gt;% \n  \n  # 4. Standardize (Z-score)\n  # Forces all genes to have Mean = 0 and SD = 1\n  step_normalize(all_predictors()) %&gt;%\n  \n  # 5. Decorrelation\n  # Removes one of two genes if they are &gt;0.9 correlated\n  step_corr(all_predictors(), threshold = 0.9)\n\n# To see the results, we 'prep' and 'bake'\nprepared_recipe &lt;- prep(bio_recipe)\ntrain_processed &lt;- bake(prepared_recipe, new_data = NULL)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Core Workflow of ML & Biological Pre-processing</span>"
    ]
  },
  {
    "objectID": "02_ml_basics.html#sec-resampling",
    "href": "02_ml_basics.html#sec-resampling",
    "title": "2  The Core Workflow of ML & Biological Pre-processing",
    "section": "2.4 2.3 Resampling: The Antidote to Overfitting",
    "text": "2.4 2.3 Resampling: The Antidote to Overfitting\nIn the \\(p \\gg n\\) regime, it is mathematically easy for a model to “memorize” the 50 patients in your training set by picking up on random noise in the 20,000 genes. To prevent this, we use V-Fold Cross-Validation (CV).\n\n2.4.1 2.3.1 How Cross-Validation Works\nInstead of training once, we divide the training set into \\(V\\) groups (folds). We train on \\(V-1\\) folds and validate on the remaining fold. We repeat this \\(V\\) times.\n\nWhy? It gives us a distribution of performance (e.g., Accuracy is \\(85\\% \\pm 3\\%\\)) rather than a single, potentially lucky number.\nBiological Context: If your CV variance is high, your “biomarkers” are likely inconsistent across different subsets of your patients.\n\nR\n# Create 10-fold cross-validation, stratified by outcome\nbio_folds &lt;- vfold_cv(train_data, v = 10, strata = outcome)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Core Workflow of ML & Biological Pre-processing</span>"
    ]
  },
  {
    "objectID": "02_ml_basics.html#sec-metrics",
    "href": "02_ml_basics.html#sec-metrics",
    "title": "2  The Core Workflow of ML & Biological Pre-processing",
    "section": "2.5 2.4 The Performance Toolbox: Beyond Accuracy",
    "text": "2.5 2.4 The Performance Toolbox: Beyond Accuracy\nFor a PhD researcher, “Accuracy” is often the least interesting metric. In biology, the cost of a mistake is asymmetric.\n\n\n\n\n\n\n\nMetric\nBiological Interpretation\n\n\nSensitivity (Recall)\nThe ability to find all truly sick patients. High sensitivity is vital for screening tests.\n\n\nSpecificity\nThe ability to correctly identify healthy patients. High specificity prevents unnecessary, invasive follow-up biopsies.\n\n\nPrecision\nOf those predicted to have the biomarker, how many actually have it? (Reduces wet-lab waste).\n\n\nAUC-ROC\nThe model’s ability to rank samples correctly regardless of the threshold.\n\n\n\n\n2.5.1 The Precision-Recall Trade-off\nIf you are identifying genes for a CRISPR screen, you might want high Precision (you don’t want to spend months testing a False Positive). If you are screening for a deadly pathogen, you want high Sensitivity (you cannot afford a False Negative).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Core Workflow of ML & Biological Pre-processing</span>"
    ]
  },
  {
    "objectID": "02_ml_basics.html#summary-the-checklist-for-chapter-2",
    "href": "02_ml_basics.html#summary-the-checklist-for-chapter-2",
    "title": "2  The Core Workflow of ML & Biological Pre-processing",
    "section": "2.6 2.5 Summary: The Checklist for Chapter 2",
    "text": "2.6 2.5 Summary: The Checklist for Chapter 2\nBefore moving to specific algorithms in the next chapters, ensure your pipeline checks these boxes:\n\nIs the data split? (Never look at the test set until the paper is written!)\nIs the normalization “leak-proof”? (Parameters calculated on training only).\nAre the features filtered? (Removed zero-variance/constant genes).\nIs the validation robust? (Used Cross-Validation to assess stability).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Core Workflow of ML & Biological Pre-processing</span>"
    ]
  },
  {
    "objectID": "02_ml_basics.html#footnotes",
    "href": "02_ml_basics.html#footnotes",
    "title": "2  The Core Workflow of ML & Biological Pre-processing",
    "section": "",
    "text": "Data Leakage occurs when information from outside the training dataset is used to create the model. This can lead to overly optimistic performance estimates because the model has effectively “seen” parts of the test data during training.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Core Workflow of ML & Biological Pre-processing</span>"
    ]
  },
  {
    "objectID": "02_ml_basics.html#the-ml-workflow-overview",
    "href": "02_ml_basics.html#the-ml-workflow-overview",
    "title": "2  The Core Workflow of ML & Biological Pre-processing",
    "section": "",
    "text": "Data Cleaning: Handling NAs and filtering low-variance features (e.g., genes with near-zero counts).\nData Spending: Splitting the data into training and testing sets to prevent data leakage.\nBiological Pre-processing: Normalizing, filtering, and transforming biological data\nModel Training: Fitting the algorithm to the training data.\nEvaluation: Testing the “unseen” data to see if the model actually learned biology or just memorized noise.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Core Workflow of ML & Biological Pre-processing</span>"
    ]
  },
  {
    "objectID": "02_ml_basics.html#sec-preprocessing",
    "href": "02_ml_basics.html#sec-preprocessing",
    "title": "2  The Core Workflow of ML & Biological Pre-processing",
    "section": "2.3 Pre-processing for Biological Data",
    "text": "2.3 Pre-processing for Biological Data\nBiological data is rarely “model-ready.” We must address two major issues: Scale and Skewness.\n\n2.3.1 Scaling and Centering\nMost ML algorithms (like SVM or Lasso) use distance-based metrics. If Gene A has expression values in the thousands and Gene B in the decimals, the model will unfairly prioritize Gene A.\n\n\n2.3.2 Near-Zero Variance (NZV)\nIn RNA-seq, many genes show little to no variation across samples. These are “noise” for a predictive model and increase the “Curse of Dimensionality.”\n\n\n2.3.3 The recipe Approach\nIn {tidymodels}, we define a “recipe” – a blueprint of transformations.\n\nlibrary(tidymodels)\n# Define a recipe for pre-processing\n# We will scale, center, and remove near-zero variance predictors\nml_recipe &lt;- recipe(sample_type ~ ., data = train_data_chol) %&gt;%\n  step_zv(all_predictors()) %&gt;%   # Remove zero variance predictors\n  step_nzv(all_predictors()) %&gt;%  # Remove near-zero variance predictors\n  step_normalize(all_predictors()) %&gt;% # Scale and center predictors  \n  step_corr(all_predictors(), threshold = 0.9) # Remove highly correlated predictors\n\n# View the recipe\nml_recipe\n\n# Prepare the recipe\n# This estimates the required parameters from the training data\nml_prep &lt;- prep(ml_recipe, training = train_data_chol)\n# Apply the pre-processing to training and testing data\ntrain_processed &lt;- bake(ml_prep, new_data = train_data_chol)\ntest_processed  &lt;- bake(ml_prep, new_data = test_data_chol)\n# Check dimensions after pre-processing\n# Near-zero variance predictors should be removed\ndim(train_processed)\n\n[1]    32 15146\n\ndim(test_processed)\n\n[1]    12 15146\n\n\nWith our data now properly split and pre-processed, we still have an issue: our training set is small. To build a robust model, we need to maximize the utility of our training data without overfitting. This is where Cross-Validation comes into play.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Core Workflow of ML & Biological Pre-processing</span>"
    ]
  },
  {
    "objectID": "02_ml_basics.html#sec-cross-val",
    "href": "02_ml_basics.html#sec-cross-val",
    "title": "2  The Core Workflow of ML & Biological Pre-processing",
    "section": "2.4 Cross-Validation: Maximizing Training Data Utility",
    "text": "2.4 Cross-Validation: Maximizing Training Data Utility\nCross-Validation (CV) is a resampling technique used to evaluate ML models on a limited data sample. The most common form is k-Fold Cross-Validation. In k-Fold CV, the training data is divided into k subsets (or “folds”). The model is trained on k-1 folds and validated on the remaining fold. This process is repeated k times, with each fold serving as the validation set once (Figure 2.2). The final performance metric is averaged across all folds.\n\n\n\nTerm\nDefinition\n\n\nFold\nA subset of the training data.\n\n\nResampling\nRepeating the split process.\n\n\nHyperparameter\nSettings of the model (not learned from data).\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.2: K-Fold Cross-Validation: The Internal Loop\n\n\n\n\n\nTo implement k-Fold CV in R, we use the vfold_cv() function from {rsample}.\n\n# Create stratified 5-fold cross-validation\nset.seed(123)\ncv_folds &lt;- vfold_cv(train_data_chol, \n                     v = 5)\n\ncv_folds\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits         id   \n  &lt;list&gt;         &lt;chr&gt;\n1 &lt;split [25/7]&gt; Fold1\n2 &lt;split [25/7]&gt; Fold2\n3 &lt;split [26/6]&gt; Fold3\n4 &lt;split [26/6]&gt; Fold4\n5 &lt;split [26/6]&gt; Fold5\n\n\n\n2.4.1 Stratified K-Fold: Ensuring Biological Representation\nAs we have discussed in Section Section 2.2, biological datasets are often imbalanced. If we use standard K-Fold CV, we risk an iteration where the “Assessment” fold accidentally contains zero cases of a specific disease subtype. Stratified K-Fold ensures that each internal fold preserves the same ratio of classes as the original training set (Figure 2.3).\n\n\n\n\n\n\n\n\nFigure 2.3: Stratified K-Fold Cross-Validation\n\n\n\n\n\nIn R, implementing this is as simple as adding the strata argument to your resampling function:\n\n# Create stratified 5-fold cross-validation\nset.seed(123)\ncv_folds &lt;- vfold_cv(train_data_chol, \n                     v = 5, \n                     strata = sample_type)\ncv_folds\n\n#  5-fold cross-validation using stratification \n# A tibble: 5 × 2\n  splits         id   \n  &lt;list&gt;         &lt;chr&gt;\n1 &lt;split [24/8]&gt; Fold1\n2 &lt;split [26/6]&gt; Fold2\n3 &lt;split [26/6]&gt; Fold3\n4 &lt;split [26/6]&gt; Fold4\n5 &lt;split [26/6]&gt; Fold5",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Core Workflow of ML & Biological Pre-processing</span>"
    ]
  },
  {
    "objectID": "02_ml_basics.html#summary-and-concluding-remarks",
    "href": "02_ml_basics.html#summary-and-concluding-remarks",
    "title": "2  The Core Workflow of ML & Biological Pre-processing",
    "section": "2.5 Summary and Concluding Remarks",
    "text": "2.5 Summary and Concluding Remarks\nIn this Chapter, we have established the “infrastructure” of a machine learning project. We moved beyond simple data loading to a rigorous pipeline that respects biological complexity and statistical integrity.\n\n2.5.1 Key Takeaways\n\nThe Mindset Shift: We moved from \\(p\\)-values (inference) to Generalization Error (prediction).\nData Spending: We learned that the Test Set is a “Locked Vault” that must remain untouched until the very end to prevent Data Leakage.\nFeature Engineering: Using {tidymodels} recipes, we automated the removal of Near-Zero Variance (NZV) genes and normalized high-throughput counts to prevent feature scale bias.\nValidation: We implemented Cross-Validation to maximize our small biological sample size (\\(n\\)) while maintaining a “mock” testing environment.\n\n\n\n2.5.2 Final Remarks\nYou now have a “processed” dataset (train_processed) and a validation strategy (cv_folds). However, we haven’t actually looked at our data yet. In the next chapter, we will explore Unsupervised Learning. We will use Dimensionality Reduction (PCA, tSNE, UMAP) to see if our biological groups (Cases vs. Controls) naturally separate before we ever try to “force” a model to learn them.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Core Workflow of ML & Biological Pre-processing</span>"
    ]
  },
  {
    "objectID": "04_ml_supervised.html",
    "href": "04_ml_supervised.html",
    "title": "4  Supervised Machine Learning",
    "section": "",
    "text": "4.1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Supervised Machine Learning</span>"
    ]
  },
  {
    "objectID": "04_ml_supervised.html#sec-regularization",
    "href": "04_ml_supervised.html#sec-regularization",
    "title": "4  Supervised Machine Learning",
    "section": "4.2 Regularization Techniques",
    "text": "4.2 Regularization Techniques",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Supervised Machine Learning</span>"
    ]
  },
  {
    "objectID": "03_ml_unsupervised.html",
    "href": "03_ml_unsupervised.html",
    "title": "3  Unsupervised Machine Learning - Dimensionality Reduction and Clustering",
    "section": "",
    "text": "3.1 The Geometry of High-Dimensional Biological Data\nTranscriptomics data typically resides in a space where \\(p \\gg n\\). Visualizing 20,000 dimensions is physically impossible; therefore, we rely on dimensionality reduction to project this information into a 2D or 3D manifold.\nThe primary objectives in this phase are:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unsupervised Machine Learning - Dimensionality Reduction and Clustering</span>"
    ]
  },
  {
    "objectID": "03_ml_unsupervised.html#the-geometry-of-high-dimensional-biological-data",
    "href": "03_ml_unsupervised.html#the-geometry-of-high-dimensional-biological-data",
    "title": "3  Unsupervised Machine Learning - Dimensionality Reduction and Clustering",
    "section": "",
    "text": "Feature Compression: Reducing the number of variables while retaining the maximum amount of biological variance.\nNoise Reduction: Filtering out stochastic variation that does not contribute to the global structure of the data.\nVisualization: Mapping samples into a coordinate system where proximity represents biological similarity.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unsupervised Machine Learning - Dimensionality Reduction and Clustering</span>"
    ]
  },
  {
    "objectID": "03_ml_unsupervised.html#principal-component-analysis-pca",
    "href": "03_ml_unsupervised.html#principal-component-analysis-pca",
    "title": "3  Unsupervised Machine Learning - Dimensionality Reduction and Clustering",
    "section": "3.2 Principal Component Analysis (PCA)",
    "text": "3.2 Principal Component Analysis (PCA)\nPCA is a linear transformation that identifies the axes (Principal Components) along which the variance of the data is maximized. It is an orthogonal transformation, meaning each subsequent component is uncorrelated with the previous ones.\n\n3.2.1 Mathematical Intuition\nEach Principal Component (\\(PC\\)) is a linear combination of the original variables:\n\\[PC_i = \\phi_{1i}X_1 + \\phi_{2i}X_2 + \\dots + \\phi_{pi}X_p\\]\nWhere \\(\\phi\\) represents the “loadings” or the weight of each gene’s contribution to that component.\n\n\n3.2.2 Implementation in tidymodels\nWe utilize the {embed} package extension for {tidymodels} to incorporate PCA directly into our reproducible pipeline.\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.4.1 ──\n\n\n✔ broom        1.0.12     ✔ recipes      1.3.1 \n✔ dials        1.4.2      ✔ rsample      1.3.2 \n✔ dplyr        1.1.4      ✔ tailor       0.1.0 \n✔ ggplot2      4.0.1      ✔ tidyr        1.3.2 \n✔ infer        1.1.0      ✔ tune         2.0.1 \n✔ modeldata    1.5.1      ✔ workflows    1.3.0 \n✔ parsnip      1.4.1      ✔ workflowsets 1.1.1 \n✔ purrr        1.2.1      ✔ yardstick    1.3.2 \n\n\nWarning: package 'broom' was built under R version 4.4.3\n\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\n\nWarning: package 'infer' was built under R version 4.4.3\n\n\nWarning: package 'parsnip' was built under R version 4.4.3\n\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\nWarning: package 'rsample' was built under R version 4.4.3\n\n\nWarning: package 'tidyr' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n\nlibrary(embed) \n\nWarning: package 'embed' was built under R version 4.4.3\n\n# Define the PCA Recipe\npca_rec &lt;- recipe(sample_type ~ ., data = train_data_chol) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_normalize(all_predictors()) %&gt;%\n  step_pca(all_predictors(), num_comp = 5)\n\n# Train the recipe on the CHOL training set\npca_estimates &lt;- prep(pca_rec)\n\n# Extract the coordinates for visualization\npca_plot_data &lt;- juice(pca_estimates)\n\n# Visualization of PC1 vs PC2\npca_plot_data %&gt;%\n  ggplot() +\n  aes(x = PC1, y = PC2, color = sample_type) +\n  geom_point(alpha = 0.7, size = 2) +\n  scale_color_manual(values = c(\"Primary Tumor\" = \"#2c7bb6\", \"Solid Tissue Normal\" = \"#d7191c\")) +\n  theme_minimal() +\n  labs(title = \"Principal Component Analysis: TCGA-CHOL\",\n       x = paste0(\"PC1 (\", round(pca_estimates$steps[[3]]$res$sdev[1]^2 / sum(pca_estimates$steps[[3]]$res$sdev^2)*100, 1), \"%)\"),\n       y = paste0(\"PC2 (\", round(pca_estimates$steps[[3]]$res$sdev[2]^2 / sum(pca_estimates$steps[[3]]$res$sdev^2)*100, 1), \"%)\"), \n       color = \"Sample Type\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nWhen we use the PCA for dimensionality reduction, we can visualize the first two principal components (PC1 and PC2) to assess the separation between “Primary Tumor” and “Solid Tissue Normal” samples. The percentage of variance explained by each component is also displayed on the axes, providing insight into how much of the original data’s variability is captured in this 2D representation. Note that, in this case, we are able to see a clear separation between the two sample types, indicating that the PCA has successfully captured the underlying structure of the data. However, PCA may not always be sufficient to capture complex non-linear relationships, which is where manifold learning techniques like t-SNE and UMAP come into play.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unsupervised Machine Learning - Dimensionality Reduction and Clustering</span>"
    ]
  },
  {
    "objectID": "03_ml_unsupervised.html#non-linear-dimensionality-reduction",
    "href": "03_ml_unsupervised.html#non-linear-dimensionality-reduction",
    "title": "3  Unsupervised Machine Learning - Dimensionality Reduction and Clustering",
    "section": "3.3 Non-Linear Dimensionality Reduction",
    "text": "3.3 Non-Linear Dimensionality Reduction\nWhile PCA is restricted to linear projections, biological systems often exhibit non-linear relationships and hierarchical structures that require manifold learning. These techniques aim to preserve the high-dimensional proximity of samples in a low-dimensional space.\n\n3.3.1 t-Distributed Stochastic Neighbor Embedding (t-SNE)\nt-SNE is a probabilistic technique specifically designed to visualize high-dimensional clusters. It converts Euclidean distances between samples into conditional probabilities that represent similarities.\n\nMechanism: It uses a Student’s t-distribution in the low-dimensional space to alleviate the “crowding problem,” where points in the center of a cluster tend to collapse onto each other.\nPerplexity: The most critical hyperparameter. It balances the attention between local and global aspects of the data. Meaning that, a low perplexity (e.g., 5) focuses on local structure, while a high perplexity (e.g., 50) captures more global relationships. Values between 5 and 50 are typical for biological datasets.\nLimitation: t-SNE often fails to preserve global structure; the distance between clusters in the plot is not always meaningful.\n\n\nlibrary(Rtsne)\n# Prepare data for t-SNE\n# Assuming 'train_data_chol' is a data frame with the sample type in the first column and gene expression in the remaining columns\n# We will use the gene expression data for t-SNE\ntsne_data &lt;- train_data_chol %&gt;%\n  select(-sample_type) %&gt;%\n  as.matrix()\n# Run t-SNE\n# Set a random seed for reproducibility\nset.seed(123)\ntsne_result &lt;- Rtsne(tsne_data, \n                     perplexity = 5, \n                     verbose = TRUE, \n                     max_iter = 1000)\n\nPerforming PCA\nRead the 32 x 32 data matrix successfully!\nUsing no_dims = 2, perplexity = 5.000000, and theta = 0.500000\nComputing input similarities...\nBuilding tree...\nDone in 0.00 seconds (sparsity = 0.636719)!\nLearning embedding...\nIteration 50: error is 57.252033 (50 iterations in 0.00 seconds)\nIteration 100: error is 54.104956 (50 iterations in 0.00 seconds)\nIteration 150: error is 62.740728 (50 iterations in 0.00 seconds)\nIteration 200: error is 61.621545 (50 iterations in 0.00 seconds)\nIteration 250: error is 65.700002 (50 iterations in 0.00 seconds)\nIteration 300: error is 1.770139 (50 iterations in 0.00 seconds)\nIteration 350: error is 1.152757 (50 iterations in 0.00 seconds)\nIteration 400: error is 0.939045 (50 iterations in 0.00 seconds)\nIteration 450: error is 0.620704 (50 iterations in 0.00 seconds)\nIteration 500: error is 0.490747 (50 iterations in 0.00 seconds)\nIteration 550: error is 0.342764 (50 iterations in 0.00 seconds)\nIteration 600: error is 0.980812 (50 iterations in 0.00 seconds)\nIteration 650: error is 0.864494 (50 iterations in 0.00 seconds)\nIteration 700: error is 0.754805 (50 iterations in 0.00 seconds)\nIteration 750: error is 0.699899 (50 iterations in 0.00 seconds)\nIteration 800: error is 0.635402 (50 iterations in 0.00 seconds)\nIteration 850: error is 0.533516 (50 iterations in 0.00 seconds)\nIteration 900: error is 0.406374 (50 iterations in 0.00 seconds)\nIteration 950: error is 0.382985 (50 iterations in 0.00 seconds)\nIteration 1000: error is 0.307605 (50 iterations in 0.00 seconds)\nFitting performed in 0.02 seconds.\n\n# Create a data frame for plotting\ntsne_plot_data &lt;- data.frame(\n  X = tsne_result$Y[, 1],\n  Y = tsne_result$Y[, 2],\n  sample_type = train_data_chol$sample_type\n)\n\n# Visualization of t-SNE results\n# Using ggplot2 for visualization\n\ntsne_plot_data %&gt;%\n  ggplot() +\n  aes(x = X, y = Y, color = sample_type) +\n  geom_point(alpha = 0.7, size = 2) +\n  scale_color_manual(values = c(\"Primary Tumor\" = \"#2c7bb6\", \n                                \"Solid Tissue Normal\" = \"#d7191c\")) +\n  theme_minimal() +\n  labs(title = \"t-SNE: TCGA-CHOL\",\n       x = \"t-SNE Dimension 1\",\n       y = \"t-SNE Dimension 2\", \n       color = \"Sample Type\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nNote that t-SNE can be computationally intensive, especially with large datasets, and the results can vary between runs if the random seed is not set. Additionally, while t-SNE can reveal clusters, it may not accurately represent the distances between those clusters, which is a critical consideration when interpreting the results in a biological context. Moreover, we can clearly see that t-SNE has successfully separated the “Primary Tumor” and “Solid Tissue Normal” samples, but the relative positioning of the clusters does not necessarily reflect their biological relationships.\n\n\n3.3.2 Uniform Manifold Approximation and Projection (UMAP)\nUMAP has largely superseded t-SNE in computational biology (particularly in single-cell transcriptomics) due to its superior speed and better preservation of global data topology.\n\n3.3.2.1 The Theoretical Foundation\nUMAP is grounded in Riemannian geometry and algebraic topology. It assumes that the data is uniformly distributed on a locally connected manifold. The algorithm constructs a fuzzy simplicial set representation of this manifold and then finds a low-dimensional layout that has the most similar topological structure.\n\n\n3.3.2.2 Advantages for Biological Data\n\nGlobal vs. Local Balance: Unlike t-SNE, the relative distances between clusters in a UMAP plot often reflect the biological distance between groups (e.g., developmental trajectories or metabolic similarity).\nPerformance: UMAP is significantly faster and more memory-efficient than t-SNE, making it suitable for large-scale multi-omic integrations.\nConsistency: UMAP is more stable across different runs, provided a random seed is set.\n\n\n\n3.3.2.3 Hyperparameter Optimization\n\nn_neighbors: This defines the size of the local neighborhood used to learn the manifold. Small values focus on fine-grained local structure (e.g., identifying rare cell subtypes), while large values focus on the global structure of the entire dataset.\nmin_dist: Determines how tightly the algorithm is allowed to pack points together. Lower values lead to tighter, more discrete clusters, while higher values preserve the broad sense of the data’s shape.\n\n\nlibrary(tidymodels)\nlibrary(embed)\n\n# Define UMAP recipe\numap_rec &lt;- recipe(sample_type ~ ., data = train_data_chol) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_normalize(all_predictors()) %&gt;%\n  step_umap(\n    all_predictors(),\n    num_comp = 2,\n    neighbors = 5,\n    min_dist = 0.1\n  )\n\n# Train recipe\numap_estimates &lt;- prep(umap_rec)\n\n# Extract coordinates\numap_plot_data &lt;- juice(umap_estimates)\n\n# Visualization\numap_plot_data %&gt;%\n  ggplot() +\n  aes(x = UMAP1, y = UMAP2, color = sample_type) +\n  geom_point(alpha = 0.7, size = 2) +\n  scale_color_manual(values = c(\"Primary Tumor\" = \"#2c7bb6\",\n                                \"Solid Tissue Normal\" = \"#d7191c\")) +\n  theme_minimal() +\n  labs(title = \"UMAP Projection: TCGA-CHOL\",\n       x = \"UMAP 1\",\n       y = \"UMAP 2\",\n       color = \"Sample Type\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nIn this UMAP visualization, we can observe that the “Primary Tumor” and “Solid Tissue Normal” samples are well-separated, and the relative distances between clusters may provide insights into their biological relationships. We do find, however, one PT within the ST cluster. UMAP’s ability to preserve both local and global structures makes it a powerful tool for exploratory data analysis in high-dimensional biological datasets.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unsupervised Machine Learning - Dimensionality Reduction and Clustering</span>"
    ]
  },
  {
    "objectID": "03_ml_unsupervised.html#manifold-learning-t-sne-and-umap",
    "href": "03_ml_unsupervised.html#manifold-learning-t-sne-and-umap",
    "title": "3  Unsupervised Machine Learning - Dimensionality Reduction and Clustering",
    "section": "3.4 Manifold Learning: t-SNE and UMAP",
    "text": "3.4 Manifold Learning: t-SNE and UMAP\nWhile PCA is restricted to linear projections, biological systems often exhibit non-linear relationships and hierarchical structures that require manifold learning. These techniques aim to preserve the high-dimensional proximity of samples in a low-dimensional space.\n\n3.4.1 t-Distributed Stochastic Neighbor Embedding (t-SNE)\nt-SNE is a probabilistic technique specifically designed to visualize high-dimensional clusters. It converts Euclidean distances between samples into conditional probabilities that represent similarities.\n\nMechanism: It uses a Student’s t-distribution in the low-dimensional space to alleviate the “crowding problem,” where points in the center of a cluster tend to collapse onto each other.\nPerplexity: The most critical hyperparameter. It balances the attention between local and global aspects of the data. Values between 5 and 50 are typical for biological datasets.\nLimitation: t-SNE often fails to preserve global structure; the distance between clusters in the plot is not always meaningful.\n\n\n\n3.4.2 3.4.2 Uniform Manifold Approximation and Projection (UMAP)\nUMAP has largely superseded t-SNE in computational biology (particularly in single-cell transcriptomics) due to its superior speed and better preservation of global data topology.\n\n3.4.2.1 The Theoretical Foundation\nUMAP is grounded in Riemannian geometry and algebraic topology. It assumes that the data is uniformly distributed on a locally connected manifold. The algorithm constructs a fuzzy simplicial set representation of this manifold and then finds a low-dimensional layout that has the most similar topological structure.\n\n\n3.4.2.2 Advantages for Biological Data\n\nGlobal vs. Local Balance: Unlike t-SNE, the relative distances between clusters in a UMAP plot often reflect the biological distance between groups (e.g., developmental trajectories or metabolic similarity).\nPerformance: UMAP is significantly faster and more memory-efficient than t-SNE, making it suitable for large-scale multi-omic integrations.\nConsistency: UMAP is more stable across different runs, provided a random seed is set.\n\n\n\n3.4.2.3 Hyperparameter Optimization\n\nn_neighbors: This defines the size of the local neighborhood used to learn the manifold. Small values focus on fine-grained local structure (e.g., identifying rare cell subtypes), while large values focus on the global structure of the entire dataset.\nmin_dist: Determines how tightly the algorithm is allowed to pack points together. Lower values lead to tighter, more discrete clusters, while higher values preserve the broad sense of the data’s shape.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unsupervised Machine Learning - Dimensionality Reduction and Clustering</span>"
    ]
  },
  {
    "objectID": "03_ml_unsupervised.html#implementation-comparing-embeddings-in-r",
    "href": "03_ml_unsupervised.html#implementation-comparing-embeddings-in-r",
    "title": "3  Unsupervised Machine Learning - Dimensionality Reduction and Clustering",
    "section": "3.5 3.5 Implementation: Comparing Embeddings in R",
    "text": "3.5 3.5 Implementation: Comparing Embeddings in R\nThe following code integrates PCA, t-SNE, and UMAP into a single comparative workflow using {tidymodels} and {embed}.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unsupervised Machine Learning - Dimensionality Reduction and Clustering</span>"
    ]
  },
  {
    "objectID": "03_ml_unsupervised.html#clustering",
    "href": "03_ml_unsupervised.html#clustering",
    "title": "3  Unsupervised Machine Learning - Dimensionality Reduction and Clustering",
    "section": "3.4 Clustering",
    "text": "3.4 Clustering\nClustering algorithms are used to group samples based on their similarity in the high-dimensional space. Common methods include hierarchical clustering, k-means, and density-based clustering (e.g., DBSCAN). These techniques can help identify subgroups of samples that may correspond to distinct biological states or disease subtypes.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unsupervised Machine Learning - Dimensionality Reduction and Clustering</span>"
    ]
  },
  {
    "objectID": "03_ml_unsupervised.html#hierarchical-clustering",
    "href": "03_ml_unsupervised.html#hierarchical-clustering",
    "title": "3  Unsupervised Machine Learning - Dimensionality Reduction and Clustering",
    "section": "3.5 Hierarchical Clustering",
    "text": "3.5 Hierarchical Clustering\nHierarchical clustering builds a tree-like structure (dendrogram) that represents the nested grouping of samples based on their pairwise distances. The choice of distance metric (e.g., Euclidean, Manhattan) and linkage method (e.g., complete, average) can significantly influence the resulting clusters.\nIn R, we can perform hierarchical clustering using the hclust function, and visualize the results with a dendrogram.\n\n# Compute distance matrix\n# A common choice for gene expression data is the Euclidean distance\ndistance_matrix &lt;- dist(\n  train_processed %&gt;% \n    select(-sample_type), \n  method = \"euclidean\")\n\n# Perform hierarchical clustering\n# We can use the complete linkage method, which considers the maximum distance between points in different clusters\nhc &lt;- hclust(distance_matrix, method = \"complete\")\n# Plot the dendrogram\n\nplot(hc, \n     labels = train_data_chol$sample_type, \n     main = \"Hierarchical Clustering Dendrogram\", \n     xlab = \"\", \n     sub = \"\")\n\n\n\n\n\n\n\n\nIn this dendrogram, samples that are more similar to each other (based on their gene expression profiles) will be grouped together. We can visually inspect the clusters to see if they correspond to the “Primary Tumor” and “Solid Tissue Normal” sample types. Additionally, we can cut the dendrogram at a specific height to define distinct clusters and analyze their biological significance.\nNote: We can also use the Hierarchical Cluter to confirm the results from the Supervised Machine Learning chapter, where we will build predictive models based on the insights gained from our unsupervised analyses. By comparing the clusters identified through hierarchical clustering with the predictions from our supervised models, we can validate the biological relevance of our findings and potentially uncover novel subtypes or patterns within the data.\n\n3.5.1 K-Means Clustering\nK-means clustering partitions the data into K distinct clusters by minimizing the within-cluster sum of squares. It is a simple and efficient algorithm but requires the number of clusters (K) to be specified in advance, which can be a limitation when the optimal number of clusters is unknown.\nA way to identify the optimal number of clusters is to use the Elbow Method, which plots the total within-cluster sum of squares against the number of clusters and looks for an “elbow” point where the rate of decrease sharply changes.\n\n# Determine the optimal number of clusters using the Elbow Method\n# We will compute the total within-cluster sum of squares for a range of K values\nwss &lt;- sapply(1:30, function(k) {\n  kmeans(train_processed %&gt;% \n           select(-sample_type), \n         centers = k, nstart = 25)$tot.withinss\n})\n# Plot the Elbow Method\nplot(1:30, wss,\n     type = \"b\", \n     las = 1, \n     pch = 19, \n     xlab = \"Number of clusters K\", \n     ylab = \"Total within-clusters sum of squares\")\n\n\n\n\n\n\n\n\nOnce we have determined the optimal number of clusters (let us use K=2), we can perform K-means clustering and visualize the results.\n\n# Perform K-means clustering with K=2\nset.seed(123)\nkmeans_result &lt;- kmeans(train_processed %&gt;% \n                          select(-sample_type), \n                        centers = 2,\n                        nstart = 25)\n# Add cluster assignments to the original data\ntrain_processed$cluster &lt;- as.factor(kmeans_result$cluster)\n# Visualize the clusters \n# We can use PCA to visualize the clusters in a 2D space\n\npca_plot_data %&gt;% \n  mutate(cluster = train_processed$cluster) %&gt;%\n  ggplot() +\n  aes(x = PC1, y = PC2, color = cluster, shape = sample_type) +\n  geom_point(alpha = 0.7, size = 2) +\n  scale_color_manual(values = c(\"1\" = \"#2c7bb6\",\n  \"2\" = \"#d7191c\")) +\n  theme_minimal() +\n  labs(title = \"K-Means Clustering (K=2) on PCA Projection\",\n       x = paste0(\"PC1 (\", round(pca_estimates$steps[[3]]$res$sdev[1]^2 / sum(pca_estimates$steps[[3]]$res$sdev^2)*100, 1), \"%)\"),\n       y = paste0(\"PC2 (\", round(pca_estimates$steps[[3]]$res$sdev[2]^2 / sum(pca_estimates$steps[[3]]$res$sdev^2)*100, 1), \"%)\"), \n       color = \"Cluster\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nIn this visualization, we can see how the K-means algorithm has partitioned the samples into two clusters. We can also compare these clusters with the original sample types (Primary Tumor vs Solid Tissue Normal) to assess how well the clustering corresponds to known biological categories. There are few misclassifications, but overall, the clusters seem to align well with the sample types, indicating that K-means has successfully captured the underlying structure of the data.\n\n\n3.5.2 Density-Based Spatial Clustering of Applications with Noise (DBSCAN)\nDBSCAN is a density-based clustering algorithm that identifies clusters based on the density of data points. It is particularly effective at identifying clusters of arbitrary shape and can handle noise (outliers) in the data. DBSCAN requires two parameters: eps, which defines the radius of the neighborhood around a point, and minPts, which specifies the minimum number of points required to form a dense region.\n\nlibrary(dbscan)\n\nWarning: package 'dbscan' was built under R version 4.4.3\n\n\n\nAttaching package: 'dbscan'\n\n\nThe following object is masked from 'package:stats':\n\n    as.dendrogram\n\n# Prepare data for DBSCAN\n# We will use the PCA-reduced data for DBSCAN to reduce computational complexity\ndbscan_data &lt;- pca_plot_data %&gt;%\n  select(PC1, PC2) %&gt;%\n  as.matrix()\n# Run DBSCAN\n# Set eps to a value that captures the local density of points and minPts to a value that reflects the expected cluster size\ndbscan_result &lt;- dbscan(dbscan_data, \n                        eps = 0.01, \n                        minPts = 1)\ndbscan_result\n\nDBSCAN clustering for 32 objects.\nParameters: eps = 0.01, minPts = 1\nUsing euclidean distances and borderpoints = TRUE\nThe clustering contains 32 cluster(s) and 0 noise points.\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 \n27 28 29 30 31 32 \n 1  1  1  1  1  1 \n\nAvailable fields: cluster, eps, minPts, metric, borderPoints\n\n# Add cluster assignments to the original data\npca_plot_data$cluster &lt;- as.factor(dbscan_result$cluster)\n# Visualize the DBSCAN clusters\npca_plot_data %&gt;%\n  ggplot() +\n  aes(x = PC1, y = PC2, color = cluster) +\n  geom_point(alpha = 0.7, size = 2) +\n  scale_color_manual(values = c(\"0\" = \"#2c7bb6\", # Cluster 0 (noise)\n                                \"1\" = \"#d7191c\", # Cluster 1\n                                \"2\" = \"#fdae61\", # Cluster 2\n                                \"3\" = \"#abdda4\", # Cluster 3\n                                \"4\" = \"#2b83ba\")) + # Cluster 4\n  theme_minimal() +\n  labs(title = \"DBSCAN Clustering on PCA Projection\",\n       x = paste0(\"PC1 (\", round(pca_estimates$steps[[3]]$res$sdev[1]^2 / sum(pca_estimates$steps[[3]]$res$sdev^2)*100, 1), \"%)\"),\n       y = paste0(\"PC2 (\", round(pca_estimates$steps[[3]]$res$sdev[2]^2 / sum(pca_estimates$steps[[3]]$res$sdev^2)*100, 1), \"%)\"), \n       color = \"Cluster\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nWhen applying DBSCAN, we cannot identify clusters based on the density of points in the PCA-reduced space. The algorithm will classify points as core points (part of a cluster), border points (on the edge of a cluster), or noise (outliers). In this example, we can see that DBSCAN has identified only one cluster, and we can compare these clusters with the original sample types to assess their biological relevance. Depending on the chosen parameters, DBSCAN may classify some samples as noise, which can be useful for identifying outliers in the data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unsupervised Machine Learning - Dimensionality Reduction and Clustering</span>"
    ]
  },
  {
    "objectID": "03_ml_unsupervised.html#conclusion",
    "href": "03_ml_unsupervised.html#conclusion",
    "title": "3  Unsupervised Machine Learning - Dimensionality Reduction and Clustering",
    "section": "3.6 Conclusion",
    "text": "3.6 Conclusion\nWe have explored various unsupervised machine learning techniques for dimensionality reduction and clustering in the context of high-dimensional biological data. PCA provided a linear projection that captured the major sources of variance, while t-SNE and UMAP offered non-linear embeddings that revealed more complex relationships between samples. Clustering algorithms like hierarchical clustering, K-means, and DBSCAN allowed us to identify groups of similar samples based on their gene expression profiles.\nWe will now proceed to the Supervised Machine Learning chapter, where we will build predictive models based on the insights gained from our unsupervised analyses.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unsupervised Machine Learning - Dimensionality Reduction and Clustering</span>"
    ]
  }
]